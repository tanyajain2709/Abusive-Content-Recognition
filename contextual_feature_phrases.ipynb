{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import the necessary methods from tweepy library\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "\n",
    "#Variables that contains the user credentials to access Twitter API \n",
    "a_token = \"718713534322872320-jtYDytc7XKSnVo7Euc2lmfHrjcDj214\"\n",
    "a_secret = \"VlCd9eGRHNefi1DpaBX0m3p3KBuTPEHqYniP1DFxghPRQ\"\n",
    "c_key = \"jmmfcpb6nNTgP8lgxkWLYgMVS\"\n",
    "c_secret = \"30OK6zanEWyh5cn4EGtr6EiUMknZtiB93YwIFI8znLa1cZiOOE\"\n",
    "\n",
    "\n",
    "#This is a basic listener that just prints received tweets to stdout.\n",
    "class Listener(StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "    \ttry:\n",
    "    \t\tprint data\n",
    "    \t\tfile=open('twitter.txt','a')\n",
    "    \t\tfile.write(data)\n",
    "    \t\tfile.write('\\n')\n",
    "    \t\tfile.close()\n",
    "    \t\treturn True\n",
    "    \texcept BaseException, e:\n",
    "    \t\tprint \"failed\",str(e)\n",
    "    \t\ttime.sleep(5)\n",
    "\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print status\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    l = Listener()\n",
    "    auth = OAuthHandler(c_key, c_secret)\n",
    "    auth.set_access_token(a_token, a_secret)\n",
    "    stream = Stream(auth, l)\n",
    "\n",
    "    #This line filter Twitter Streams to capture data by the keywords: 'python', 'javascript', 'ruby'\n",
    "    #stream.filter(track=['python', 'javascript', 'ruby']),stupid,fuck,that,from,mother,shit,wtf,suck,cunt\n",
    "    stream.filter(track=[\"#AntiTrump\",'#NotMyPresident','#FuckYouTrump','#BoycottTrump','#NeverTrump','#DrumpTrump'],languages=['en'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.corpus\n",
    "import string\n",
    "import re , math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#slang dict creation\n",
    "f = open('/home/tanya/Desktop/major/after_phase1/analyse/classifiers/slang.txt', 'r')\n",
    "slang_dict = {}\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    parts = [p.strip() for p in line.split(\"=\")]\n",
    "    slang_dict[parts[0].lower()] = (parts[1].lower())\n",
    "f.close()\n",
    "#print slang_dict\n",
    "\n",
    "#creation of abusive words list\n",
    "file1 = open('/home/tanya/Desktop/major/after_phase1/analyse/classifiers/abusive.txt')\n",
    "abusive_words = list()\n",
    "line = file1.readline()\n",
    "while line:\n",
    "    word = line.rstrip()\n",
    "    abusive_words.append(word)\n",
    "    line=file1.readline()\n",
    "file1.close()\n",
    "\n",
    "#creation of less offensive dict\n",
    "file1 = open('/home/tanya/Desktop/major/after_phase1/analyse/classifiers/less_offensive.txt')\n",
    "less_offensive = list()\n",
    "line = file1.readline()\n",
    "while line:\n",
    "    word = line.rstrip()\n",
    "    less_offensive.append(word)\n",
    "    line=file1.readline()\n",
    "file1.close()\n",
    "#stop list\n",
    "stop_list = nltk.corpus.stopwords.words('english')\n",
    "stop_list.extend(string.punctuation)\n",
    "stop_list.append('atuser')\n",
    "stop_list.append('url')\n",
    "stop_list.append('rt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir(\"/home/tanya/Desktop/major/spark\")\n",
    "os.curdir\n",
    "if 'SPARK_HOME' not in os.environ:\n",
    "    os.environ['SPARK_HOME']='/home/tanya/spark-2.0.0-bin-hadoop2.4'\n",
    "SPARK_HOME=os.environ['SPARK_HOME']\n",
    "\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\",\"pyspark.zip\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\",\"py4j-0.10.1-src.zip\"))\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "conf=SparkConf()\n",
    "conf.set(\"spark.executor.memory\",\"1g\")\n",
    "conf.set(\"spark.cores.max\",\"1\")\n",
    "\n",
    "conf.setAppName(\"major\")\n",
    "\n",
    "sc = SparkContext('local',conf=conf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "import enchant\n",
    "import editdistance \n",
    "d = enchant.Dict(\"en_US\")\n",
    "sqlContext = SQLContext(sc)\n",
    "import re ,string\n",
    "def preprocess(x):\n",
    "    l = x.split(',')\n",
    "    if(len(l)>=4):\n",
    "        #convert to lower case\n",
    "        text = str(l[3].decode('utf-8')).lower()\n",
    "        text=text.replace('\"text\":', '')\n",
    "        \n",
    "        #2)standardising slang words\n",
    "        \n",
    "        l1 = text.split(\" \")\n",
    "        if '' in l1:\n",
    "            l1.remove('')\n",
    "        for i in range(len(l1)):\n",
    "            if l1[i] in slang_dict:\n",
    "                l1[i] = slang_dict[l1[i]]\n",
    "        text=' '.join(l1)\n",
    "        \n",
    "        #covert @user to at_user\n",
    "        text= re.sub('@[^\\s]+','atuser',text)\n",
    "        \n",
    "        #covert #data to data\n",
    "        text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "        \n",
    "        #Remove additional white spaces\n",
    "        text = re.sub('[\\s]+', ' ', text)\n",
    "        \n",
    "        text  = text.replace('\\\\', '')\n",
    "        #replace url by 'url'\n",
    "        text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','url',text)\n",
    "        \n",
    "         #--feature vector--#\n",
    "\n",
    "        #1)removing punctuation marks\n",
    "        text =  text.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "        \n",
    "         #3) removing repeated chars\n",
    "        pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "        text = pattern.sub(r\"\\1\\1\", text)\n",
    "        \n",
    "        #3) removing words containing no\n",
    "        text=re.sub(r'\\w*\\d\\w*', '', text).strip()\n",
    "       \n",
    "       \n",
    "        #4) remove stop words\n",
    "        li = text.split(\" \")\n",
    "        for word in stop_list:\n",
    "            for token in li:\n",
    "                if token == word:\n",
    "                    li.remove(word)\n",
    "\n",
    "        text=\" \".join(li)\n",
    "        \n",
    "        li = text.split(\" \")\n",
    "        for word in stop_list:\n",
    "            for token in li:\n",
    "                if token == word:\n",
    "                    li.remove(word)\n",
    "\n",
    "        text=\" \".join(li)\n",
    "        \n",
    "        #5)spell correction\n",
    "        skip = True\n",
    "        suggestion_list = []\n",
    "        distances = []\n",
    "        data = []\n",
    "        words=text.split(\" \")\n",
    "        for k in range(len(words)):\n",
    "            if ' ' in words:\n",
    "                words.remove(' ')\n",
    "        for k in range(len(words)):\n",
    "            if  words[k]!='':\n",
    "                \n",
    "            \n",
    "                if(d.check(words[k])==False):\n",
    "                    suggestion=d.suggest(words[k])\n",
    "                    if len(suggestion)!=0:\n",
    "                        for i in range(0,len(suggestion)):\n",
    "                            distance = editdistance.eval(words[k],suggestion[i])\n",
    "                            distances.append(distance)\n",
    "                            suggestion_list.append(suggestion[i])\n",
    "                        index = distances.index(min(distances))\n",
    "                        words[k]=suggestion_list[index]\n",
    "            distances = []\n",
    "            suggestion_list = []\n",
    "        text=\" \".join(words)\n",
    "        \n",
    "        \n",
    "        return Row(tweet=text)\n",
    "        \n",
    "    else:\n",
    "        return Row(tweet=\"not there\")\n",
    "        \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process1(x):\n",
    "    for term in abusive_words:\n",
    "        if term in x[0]:\n",
    "            return x\n",
    "def process2(x):\n",
    "    for term in less_offensive:\n",
    "        if term in x[0]:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "452"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.textFile('/home/tanya/Desktop/major/after_phase1/analyse/classifiers/twitter.txt')\n",
    "rdd1.cache()\n",
    "rdd1.collect()\n",
    "from pyspark.sql import Row\n",
    "\n",
    "final = rdd1.map(preprocess)\n",
    "final.collect()\n",
    "final.cache()\n",
    "#final.count()\n",
    "#final1.collect()\n",
    "final1 = final.filter(lambda x:x.tweet!=\"not there\")\n",
    "df1=sc.parallelize(final1.collect())\n",
    "df1=df1.toDF()\n",
    "df1=df1.dropDuplicates([\"tweet\"])\n",
    "#final1=final1.distinct()\n",
    "final1=df1.rdd\n",
    "final1.cache()\n",
    "final1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "415"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abusive_tweets = final1.filter(process1)\n",
    "abusive_tweets.count()\n",
    "final1=final1.subtract(abusive_tweets)\n",
    "final1.cache()\n",
    "final1.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('/home/tanya/Desktop/major/after_phase1/analyse/classifiers/phrase.txt', 'w')\n",
    "for x in final1.collect():\n",
    "    f.write(x[0])\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
