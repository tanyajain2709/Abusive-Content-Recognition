{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.corpus\n",
    "import string\n",
    "import re , math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#slang dict creation\n",
    "f = open('/home/tanya/Desktop/major/after_phase1/analyse/classifiers/slang.txt', 'r')\n",
    "slang_dict = {}\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    parts = [p.strip() for p in line.split(\"=\")]\n",
    "    slang_dict[parts[0].lower()] = (parts[1].lower())\n",
    "f.close()\n",
    "#print slang_dict\n",
    "\n",
    "#creation of abusive words list\n",
    "file1 = open('/home/tanya/Desktop/major/after_phase1/analyse/classifiers/abusive.txt')\n",
    "abusive = list()\n",
    "line = file1.readline()\n",
    "while line:\n",
    "    word = line.rstrip()\n",
    "    abusive.append(word)\n",
    "    line=file1.readline()\n",
    "file1.close()\n",
    "\n",
    "#creation of less offensive dict\n",
    "file1 = open('/home/tanya/Desktop/major/after_phase1/analyse/classifiers/less_offensive.txt')\n",
    "less_offensive = list()\n",
    "line = file1.readline()\n",
    "while line:\n",
    "    word = line.rstrip()\n",
    "    less_offensive.append(word)\n",
    "    line=file1.readline()\n",
    "file1.close()\n",
    "#stop list\n",
    "stop_list = nltk.corpus.stopwords.words('english')\n",
    "stop_list.extend(string.punctuation)\n",
    "stop_list.append('atuser')\n",
    "stop_list.append('url')\n",
    "stop_list.append('rt')\n",
    "\n",
    "#phrase_dict\n",
    "file1 = open('/home/tanya/Desktop/major/after_phase1/analyse/classifiers/phrase.txt')\n",
    "phrase_list = list()\n",
    "line = file1.readline()\n",
    "while line:\n",
    "    word = line.rstrip()\n",
    "    phrase_list.append(word)\n",
    "    line=file1.readline()\n",
    "file1.close()\n",
    "\n",
    "#adjective list\n",
    "file1 = open('/home/tanya/Desktop/major/after_phase1/analyse/classifiers/adjectives.txt')\n",
    "adj = list()\n",
    "line = file1.readline()\n",
    "while line:\n",
    "    word = line.rstrip()\n",
    "    adj.append(word)\n",
    "    line=file1.readline()\n",
    "file1.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir(\"/home/tanya/Desktop/major/spark\")\n",
    "os.curdir\n",
    "if 'SPARK_HOME' not in os.environ:\n",
    "    os.environ['SPARK_HOME']='/home/tanya/spark-2.0.0-bin-hadoop2.4'\n",
    "SPARK_HOME=os.environ['SPARK_HOME']\n",
    "\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\",\"pyspark.zip\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\",\"py4j-0.10.1-src.zip\"))\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "conf=SparkConf()\n",
    "conf.set(\"spark.executor.memory\",\"1g\")\n",
    "conf.set(\"spark.cores.max\",\"1\")\n",
    "\n",
    "conf.setAppName(\"major\")\n",
    "\n",
    "sc = SparkContext('local',conf=conf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "import enchant\n",
    "import editdistance \n",
    "d = enchant.Dict(\"en_US\")\n",
    "sqlContext = SQLContext(sc)\n",
    "import re ,string\n",
    "def preprocess(x):\n",
    "    l = x.split(',')\n",
    "    if(len(l)>=4):\n",
    "        #convert to lower case\n",
    "        text = str(l[3].decode('utf-8')).lower()\n",
    "        text=text.replace('\"text\":', '')\n",
    "        \n",
    "        #2)standardising slang words\n",
    "        \n",
    "        l1 = text.split(\" \")\n",
    "        if '' in l1:\n",
    "            l1.remove('')\n",
    "        for i in range(len(l1)):\n",
    "            if l1[i] in slang_dict:\n",
    "                l1[i] = slang_dict[l1[i]]\n",
    "        text=' '.join(l1)\n",
    "        \n",
    "        #covert @user to at_user\n",
    "        text= re.sub('@[^\\s]+','atuser',text)\n",
    "        \n",
    "        #covert #data to data\n",
    "        text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "        \n",
    "        #Remove additional white spaces\n",
    "        text = re.sub('[\\s]+', ' ', text)\n",
    "        \n",
    "        text  = text.replace('\\\\', '')\n",
    "        #replace url by 'url'\n",
    "        text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','url',text)\n",
    "        \n",
    "         #--feature vector--#\n",
    "\n",
    "        #1)removing punctuation marks\n",
    "        text =  text.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "        \n",
    "         #3) removing repeated chars\n",
    "        pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "        text = pattern.sub(r\"\\1\\1\", text)\n",
    "        \n",
    "        #3) removing words containing no\n",
    "        text=re.sub(r'\\w*\\d\\w*', '', text).strip()\n",
    "       \n",
    "                \n",
    "        \n",
    "        sid = list(str(l[1]))[5:]\n",
    "        sid = int(''.join(sid))\n",
    "        \n",
    "        return Row(tweet=str(text),tid=sid, status=0)\n",
    "        \n",
    "    else:\n",
    "        return Row(tweet=\"not there\",tid=0, status=0)\n",
    "        \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.textFile('/home/tanya/Desktop/major/after_phase1/dataset/dataset.txt')\n",
    "rdd1.cache()\n",
    "from pyspark.sql import Row\n",
    "\n",
    "final = rdd1.map(preprocess)\n",
    "final.cache()\n",
    "df = sc.parallelize(final.collect())\n",
    "df=df.toDF()\n",
    "df=df.dropDuplicates([\"tweet\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "final1 = df.rdd\n",
    "final1 = final1.filter(lambda x:x.tweet!=\"not there\")\n",
    "final1=final1.filter(lambda x:x.tweet!='')\n",
    "#final1=final1.filter(lambda x:len(x.split(' ')>1))\n",
    "final1.cache()\n",
    "final1.count()\n",
    "read_rdd = final1 ##read_id for mapping\n",
    "read_rdd.take(4)\n",
    "read_rdd.saveAsTextFile(\"/home/tanya/Desktop/major/after_phase1/analyse/classifiers/rdd_files/all_tweets.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def proc(x):\n",
    "    status = x[0]\n",
    "    sid = x[1]\n",
    "    tweet = str(x[2])\n",
    "    tweet = tweet.split(\" \")\n",
    "    tweet1 = [x for x in tweet if x not in stop_list ]\n",
    "    tweet1=\" \".join(tweet1)\n",
    "    \n",
    "    #spell correction\n",
    "    skip = True\n",
    "    suggestion_list = []\n",
    "    distances = []\n",
    "    data = []\n",
    "    words=tweet1.split(\" \")\n",
    "    for k in range(len(words)):\n",
    "        if ' ' in words:\n",
    "            words.remove(' ')\n",
    "    for k in range(len(words)):\n",
    "        if  words[k]!='':\n",
    "\n",
    "\n",
    "            if(d.check(words[k])==False):\n",
    "                suggestion=d.suggest(words[k])\n",
    "                if len(suggestion)!=0:\n",
    "                    for i in range(0,len(suggestion)):\n",
    "                        distance = editdistance.eval(words[k],suggestion[i])\n",
    "                        distances.append(distance)\n",
    "                        suggestion_list.append(suggestion[i])\n",
    "                    index = distances.index(min(distances))\n",
    "                    words[k]=suggestion_list[index]\n",
    "        distances = []\n",
    "        suggestion_list = []\n",
    "    tweet1=\" \".join(words)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    return Row(tid=sid,tweet=str(tweet1),status=status)\n",
    "final1 = final1.map(proc)\n",
    "final1.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#func to separate out abusive tweets\n",
    "def process(x):\n",
    "        for term in abusive:\n",
    "            if term in x[2].split(\" \"):\n",
    "                return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create an abusive rdd from process func\n",
    "abusive_tweets = final1.filter(process)\n",
    "abusive_tweets.cache()\n",
    "abusive_tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#write abusive tweets to file \n",
    "f = open(\"/home/tanya/Desktop/major/after_phase1/analyse/classifiers/rdd_files/abusive_tweets.csv\",\"w\")\n",
    "for x in abusive_tweets.collect():\n",
    "    for y in read_rdd.collect():\n",
    "        if x[1] == y[1]:\n",
    "            f.write(str(y))\n",
    "            f.write('\\n')\n",
    "f.close()           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "refined_tweets = final1.subtract(abusive_tweets)\n",
    "refined_tweets.cache()\n",
    "refined_tweets.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#function to filter out less offensive tweets\n",
    "def filter_lessoffensive(x):\n",
    "        for term in less_offensive:\n",
    "            if term in x[2].split(\" \"):\n",
    "                return x\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "non_neutral = refined_tweets.filter(filter_lessoffensive)\n",
    "non_neutral.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#write nn tweets to file \n",
    "f = open(\"/home/tanya/Desktop/major/after_phase1/analyse/classifiers/rdd_files/nn_tweets.csv\",\"w\")\n",
    "for x in non_neutral.collect():\n",
    "    for y in read_rdd.collect():\n",
    "        if x[1] == y[1]:\n",
    "            f.write(str(y))\n",
    "            f.write('\\n')\n",
    "f.close()           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neutral_tweets =refined_tweets.subtract(non_neutral)    \n",
    "#write neutral tweets to file \n",
    "f = open(\"/home/tanya/Desktop/major/after_phase1/analyse/classifiers/rdd_files/neutral_tweets.csv\",\"w\")\n",
    "for x in neutral_tweets.collect():\n",
    "    for y in read_rdd.collect():\n",
    "        if x[1] == y[1]:\n",
    "            f.write(str(y))\n",
    "            f.write('\\n')\n",
    "f.close()\n",
    "#neutral_tweets.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1885"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neutral_tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########context based classification########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1885"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## after context eval rdd\n",
    "def proc_change(x):\n",
    "    l = str(x).split(\",\")\n",
    "    #print l\n",
    "    status = int((list(l[0])[11:][0]))\n",
    "    #print status\n",
    "    tid = int(l[1][5:])\n",
    "    #print tid\n",
    "    tweet = list(l[2])[9:]\n",
    "    #print tweet\n",
    "    tweet.pop(len(tweet)-1)\n",
    "    tweet.pop(len(tweet)-1)\n",
    "    tweet=''.join(tweet)\n",
    "    return(Row(status=status,tid=tid,tweet=tweet))\n",
    "\n",
    "neutral_tweets1 = sc.textFile('/home/tanya/Desktop/major/after_phase1/analyse/classifiers/rdd_files/neutral_tweets.csv')\n",
    "neutral_tweets1=neutral_tweets1.map(proc_change)\n",
    "neutral_tweets1.count()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####cosine similarity#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#functions for calculating cosine similarity\n",
    "word = re.compile(r'\\w')\n",
    "def cal_cosine(v1,v2):\n",
    "    for  term in stop_list:\n",
    "        if term in v1:\n",
    "            del v1[term]\n",
    "        if term in v2:\n",
    "            del v2[term]\n",
    "    set1 = v1.keys()\n",
    "    set2 = v2.keys()\n",
    "    num = sum([v1[x] * v2[x] for x in (set1 and set2)])\n",
    "    \n",
    "    prod1 = math.sqrt(sum(v1[x]**2 for x in v1.keys()))\n",
    "    prod2 = math.sqrt(sum(v2[x]**2 for x in v2.keys()))\n",
    "    den = prod1 * prod2\n",
    "    \n",
    "    if not den:\n",
    "        return \n",
    "    else:\n",
    "        return float(num) /den\n",
    "    \n",
    "def calc_vector(sentence):\n",
    "    return Counter(sentence.split(\" \"))\n",
    "\n",
    "##function to calculate the vector of all the phrases in dict\n",
    "def calc_vector_phrase():\n",
    "    phrase_vector=list()\n",
    "    for phrase in phrase_list:\n",
    "           phrase_vector.append(calc_vector(phrase))\n",
    "    return phrase_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_cosine(x):\n",
    "    v1 = calc_vector(x[2])\n",
    "    phrase_vector = calc_vector_phrase()\n",
    "    for v2 in phrase_vector:\n",
    "        cos = cal_cosine(v1,v2)\n",
    "        if cos>0.8:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##phrase_cos_offensive is rdd containg the cosine rule offensive tweets\n",
    "phrase_cos_offensive = neutral_tweets1.filter(filter_cosine)\n",
    "phrase_cos_offensive.saveAsTextFile('/home/tanya/Desktop/major/after_phase1/analyse/classifiers/rdd_files/cos_offensive.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# saving the clean tweets lleft after cosine in textfile\n",
    "neutral_tweets1.subtract(phrase_cos_offensive).saveAsTextFile('/home/tanya/Desktop/major/after_phase1/analyse/classifiers/rdd_files/cos_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "###classification based on adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def filter_adjective(x):\n",
    "    for term in adj :\n",
    "        if term in x[2].split(\" \"):\n",
    "            return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1439"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj = sorted(set(adj))\n",
    "adjective_offensive = neutral_tweets1.filter(filter_adjective)\n",
    "adjective_offensive.saveAsTextFile('/home/tanya/Desktop/major/after_phase1/analyse/classifiers/rdd_files/adj_off.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neutral_tweets1.subtract(adjective_offensive).saveAsTextFile('/home/tanya/Desktop/major/after_phase1/analyse/classifiers/rdd_files/adj_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
